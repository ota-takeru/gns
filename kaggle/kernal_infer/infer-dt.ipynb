{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e989b71f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# Kaggle の入力データセット slug（username/slug の slug 部分）\n",
        "DATASET_SLUG = 'gns-codes'\n",
        "DATASET_ROOT = Path(f'/kaggle/input/{DATASET_SLUG}')\n",
        "WORK_ROOT = Path('/kaggle/working')\n",
        "repo_dir = WORK_ROOT / 'code'\n",
        "\n",
        "code_dir = DATASET_ROOT / 'code'\n",
        "code_zip = DATASET_ROOT / 'code.zip'\n",
        "\n",
        "# 展開済み code/ を優先し、無ければ code.zip を展開\n",
        "if code_dir.exists():\n",
        "    src = code_dir\n",
        "elif code_zip.exists():\n",
        "    if repo_dir.exists():\n",
        "        shutil.rmtree(repo_dir)\n",
        "    with zipfile.ZipFile(code_zip) as zf:\n",
        "        zf.extractall(repo_dir)\n",
        "    src = repo_dir\n",
        "else:\n",
        "    raise FileNotFoundError(f\"/kaggle/input/{DATASET_SLUG} に code も code.zip もありません。Add Data で {DATASET_SLUG} を追加してください。\")\n",
        "\n",
        "# フラット化（zip解凍で1階層挟まった場合）\n",
        "if src != repo_dir:\n",
        "    if repo_dir.exists():\n",
        "        shutil.rmtree(repo_dir)\n",
        "    shutil.copytree(src, repo_dir)\n",
        "if repo_dir.exists():\n",
        "    children = list(repo_dir.iterdir())\n",
        "    if len(children) == 1 and children[0].is_dir():\n",
        "        inner = children[0]\n",
        "        for p in inner.iterdir():\n",
        "            p.rename(repo_dir / p.name)\n",
        "        inner.rmdir()\n",
        "\n",
        "%cd $repo_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bf889a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# PyG の radius_graph で必要になる torch-cluster だけインストール\n",
        "import torch\n",
        "\n",
        "torch_ver = torch.__version__.split('+')[0]\n",
        "cuda_ver = torch.version.cuda\n",
        "cuda_tag = 'cpu' if cuda_ver is None else 'cu' + cuda_ver.replace('.', '')\n",
        "url = f\"https://data.pyg.org/whl/torch-{torch_ver}+{cuda_tag}.html\"\n",
        "print('PyG wheels:', url)\n",
        "\n",
        "!pip -q install torch-cluster -f {url}\n",
        "!pip -q install torch_geometric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b81493ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import copy\n",
        "import yaml\n",
        "import re\n",
        "import json\n",
        "\n",
        "REPO = Path('/kaggle/working/code')\n",
        "cfg_path = REPO / 'config_rollout.yaml'\n",
        "\n",
        "# モデルごとに対応するデータセットを自動解決する\n",
        "\n",
        "def _load_metadata(path: Path):\n",
        "    try:\n",
        "        with path.open('r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _find_metadata(root: Path):\n",
        "    candidates = [\n",
        "        root / 'metadata.json',\n",
        "        root / 'metadata' / 'metadata.json',\n",
        "    ]\n",
        "    candidates += list(root.glob('**/metadata*.json'))\n",
        "    for c in candidates:\n",
        "        if c.is_file():\n",
        "            meta = _load_metadata(c)\n",
        "            if isinstance(meta, dict):\n",
        "                return meta, c\n",
        "    return None, None\n",
        "\n",
        "def _extract_sequence_length(meta: dict):\n",
        "    if not isinstance(meta, dict):\n",
        "        return None\n",
        "    for key in ('train', 'valid', 'test', 'rollout'):\n",
        "        part = meta.get(key)\n",
        "        if isinstance(part, dict) and 'sequence_length' in part:\n",
        "            try:\n",
        "                return int(part['sequence_length'])\n",
        "            except Exception:\n",
        "                pass\n",
        "    if 'sequence_length' in meta:\n",
        "        try:\n",
        "            return int(meta['sequence_length'])\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def resolve_dataset_root(step: int | None):\n",
        "    base = Path('/kaggle/input')\n",
        "    if not base.exists():\n",
        "        return None, False\n",
        "\n",
        "    # 1) ディレクトリ名にステップ数が含まれるものを優先\n",
        "    if step is not None:\n",
        "        for dataset_root in sorted(base.iterdir()):\n",
        "            name = dataset_root.name\n",
        "            if (name.endswith(f'-{step}') or name.endswith(f'_{step}')\n",
        "                    or f'-{step}-' in name or f'_{step}_' in name):\n",
        "                return dataset_root, False\n",
        "\n",
        "    # 2) metadata の sequence_length で一致を探す\n",
        "    for dataset_root in sorted(base.iterdir()):\n",
        "        meta, meta_path = _find_metadata(dataset_root)\n",
        "        if not meta:\n",
        "            continue\n",
        "        seq = _extract_sequence_length(meta)\n",
        "        if step is not None and seq == step:\n",
        "            return dataset_root, False\n",
        "\n",
        "    # 3) フォールバック（もっとも汎用な800データセット）\n",
        "    fallback = base / 'dam-break-left-800'\n",
        "    if fallback.exists():\n",
        "        print(f\"[fallback] step={step} に一致するデータセットが見つからず、{fallback} を使用します。\")\n",
        "        return fallback, True\n",
        "    print(f\"[fallback] step={step} に一致するデータセットも既定の dam-break-left-800 も見つかりません。\")\n",
        "    return None, True\n",
        "\n",
        "# 出力設定（共通）\n",
        "output_root = REPO / 'rollouts'\n",
        "viz_format = 'html'  # html|mp4|gif\n",
        "\n",
        "with cfg_path.open('r', encoding='utf-8') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# データセットはモデルごとに後続セルで差し替える\n",
        "cfg['method'] = 'gns'\n",
        "cfg['rollout_inference_max_examples'] = None\n",
        "cfg['output_path'] = str(output_root)\n",
        "cfg.setdefault('scenario_options', {}).setdefault('fluid', {})['dataset'] = '/kaggle/input'  # placeholder\n",
        "\n",
        "# モデルは後続セルで差し替える（ベースを保存しておく）\n",
        "cfg['model_path'] = str(REPO / 'models')\n",
        "cfg['model_file'] = None\n",
        "cfg['output_filename'] = 'rollout'\n",
        "\n",
        "BASE_CFG = copy.deepcopy(cfg)\n",
        "\n",
        "with cfg_path.open('w', encoding='utf-8') as f:\n",
        "    yaml.safe_dump(cfg, f, allow_unicode=True)\n",
        "\n",
        "print('ベース設定を書き出しました:', cfg_path)\n",
        "print('output_path:', cfg['output_path'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0a373b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "%cd /kaggle/working/code\n",
        "\n",
        "import copy\n",
        "import re\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "from analyze_rollouts import analyze_rollout\n",
        "\n",
        "# 比較対象モデルを集約（rollout_diff 優先）\n",
        "model_roots = [\n",
        "    REPO / 'models' / 'rollout_diff',\n",
        "    REPO / 'models',\n",
        "]\n",
        "model_files: list[Path] = []\n",
        "for root in model_roots:\n",
        "    if not root.exists():\n",
        "        continue\n",
        "    for path in root.glob('model-*.pt'):\n",
        "        model_files.append(path.resolve())\n",
        "\n",
        "# 重複除去 & ソート（親ディレクトリ優先→ステップ番号）\n",
        "seen = set()\n",
        "unique_models: list[Path] = []\n",
        "for p in model_files:\n",
        "    if p in seen:\n",
        "        continue\n",
        "    seen.add(p)\n",
        "    unique_models.append(p)\n",
        "\n",
        "def _step_key(p: Path):\n",
        "    m = re.search(r'(\\d+)', p.stem)\n",
        "    step = int(m.group(1)) if m else -1\n",
        "    priority = 0 if p.parent.name == 'rollout_diff' else 1\n",
        "    return (priority, p.parent.name, step)\n",
        "\n",
        "def _step_from_path(p: Path):\n",
        "    m = re.search(r'(\\d+)(?!.*\\d)', p.stem)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "model_files = sorted(unique_models, key=_step_key)\n",
        "\n",
        "if not model_files:\n",
        "    raise FileNotFoundError('model-*.pt が見つかりません。models/ 下に配置してください。')\n",
        "\n",
        "print('評価対象モデル:')\n",
        "for p in model_files:\n",
        "    print(' -', p)\n",
        "\n",
        "results = []\n",
        "\n",
        "cmd = ['python', 'src/train.py', '--config', str(cfg_path)]\n",
        "\n",
        "for model_path in model_files:\n",
        "    tag = f\"{model_path.parent.name}-{model_path.stem}\"\n",
        "    step_num = _step_from_path(model_path)\n",
        "    dataset_root, used_fallback = resolve_dataset_root(step_num)\n",
        "    if dataset_root is None:\n",
        "        raise FileNotFoundError(f\"対応するデータセットが見つかりません（step={step_num}）\")\n",
        "\n",
        "    cfg = copy.deepcopy(BASE_CFG)\n",
        "    cfg['model_path'] = str(model_path.parent)\n",
        "    cfg['model_file'] = str(model_path)\n",
        "    cfg['output_filename'] = f\"rollout_{tag}\"\n",
        "    cfg.setdefault('scenario_options', {}).setdefault('fluid', {})['dataset'] = str(dataset_root)\n",
        "    cfg['data_path'] = str(dataset_root)\n",
        "    cfg['rollout_dataset'] = 'valid'\n",
        "\n",
        "    with cfg_path.open('w', encoding='utf-8') as f:\n",
        "        yaml.safe_dump(cfg, f, allow_unicode=True)\n",
        "\n",
        "    print(f\"\\n=== {tag} ===\")\n",
        "    print('dataset:', dataset_root)\n",
        "    if used_fallback:\n",
        "        print('[warn] 指定ステップに一致するデータセットが見つからなかったためフォールバックを使用しました。')\n",
        "    print('Running:', ' '.join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "    output_dir = Path(cfg['output_path']) / cfg['method'] / cfg['output_filename']\n",
        "    pkl_files = sorted(output_dir.glob(f\"{cfg['output_filename']}_ex*.pkl\"))\n",
        "    if not pkl_files:\n",
        "        raise FileNotFoundError(f\"{output_dir} に *_ex*.pkl がありません。推論が成功したか確認してください。\")\n",
        "\n",
        "    dist_list = []\n",
        "    for pkl_path in pkl_files:\n",
        "        res_one = analyze_rollout(pkl_path)\n",
        "        dist_list.append(res_one['distance_error_per_timestep'])\n",
        "\n",
        "    min_len = min(len(a) for a in dist_list)\n",
        "    dist_stack = np.stack([a[:min_len] for a in dist_list])\n",
        "    avg_dist = dist_stack.mean(axis=0)\n",
        "    mean_full = float(avg_dist.mean())\n",
        "\n",
        "    results.append({\n",
        "        'tag': tag,\n",
        "        'distance_error_per_timestep': avg_dist.tolist(),\n",
        "        'mean_distance_error': mean_full,\n",
        "        'n_rollouts': len(dist_list),\n",
        "        'used_length': min_len,\n",
        "        'pkl_dir': str(output_dir),\n",
        "        'dataset_root': str(dataset_root),\n",
        "        'step_num': step_num,\n",
        "        'used_fallback': used_fallback,\n",
        "    })\n",
        "\n",
        "    print(f\"  rollouts: {len(dist_list)} / used length: {min_len} / mean distance error: {mean_full:.6f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e8e839",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "MAX_SUMMARY_STEPS = 100\n",
        "# None なら上限なし（従来は 0 で無表示になっていた）\n",
        "MAX_PLOT_STEPS = None  # 1ステップ誤差可視化の表示上限\n",
        "# モデルごとに評価を打ち切る最大ステップ数を指定（なければ全ステップ）\n",
        "STEP_LIMITS = {\n",
        "    'rollout_diff-model-100': 100,\n",
        "    'rollout_diff-model-200': 200,\n",
        "    'rollout_diff-model-400': 400,\n",
        "    'rollout_diff-model-800': 800,\n",
        "}\n",
        "AUTO_LIMIT_FROM_TAG = True  # タグ末尾の数値 (例: model-400) を上限として使う\n",
        "\n",
        "def resolve_limit(tag: str, total_len: int) -> int:\n",
        "    if tag in STEP_LIMITS:\n",
        "        return min(STEP_LIMITS[tag], total_len)\n",
        "    if AUTO_LIMIT_FROM_TAG:\n",
        "        m = re.search(r'(\\d+)(?!.*\\d)', tag)\n",
        "        if m:\n",
        "            return min(int(m.group(1)), total_len)\n",
        "    return total_len\n",
        "\n",
        "if not results:\n",
        "    print('results が空です。前のセルを実行してください。')\n",
        "else:\n",
        "    # -------- 全タイムステップの距離誤差（従来のラインプロット） --------\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for r in results:\n",
        "        err = np.array(r['distance_error_per_timestep'])\n",
        "        limit = resolve_limit(r['tag'], len(err))\n",
        "        err = err[:limit]\n",
        "        timesteps = np.arange(len(err))\n",
        "        plt.plot(\n",
        "            timesteps,\n",
        "            err,\n",
        "            label=f\"{r['tag']} (T={len(err)})\",\n",
        "        )\n",
        "    plt.xlabel('timestep')\n",
        "    plt.ylabel('mean distance error')\n",
        "    plt.title('Rollout distance error per timestep')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plot_path = output_root / 'distance_error_comparison.png'\n",
        "    plt.savefig(plot_path, dpi=150)\n",
        "    plt.show()\n",
        "    print('プロットを保存:', plot_path)\n",
        "\n",
        "    # -------- 1ステップ誤差（タイムステップごと、散布＋線） --------\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for r in results:\n",
        "        err = np.array(r['distance_error_per_timestep'])\n",
        "        limit = resolve_limit(r['tag'], len(err))\n",
        "        if MAX_PLOT_STEPS is not None:\n",
        "            limit = min(limit, MAX_PLOT_STEPS)\n",
        "        timesteps = np.arange(limit)\n",
        "        plt.plot(timesteps, err[:limit], marker='o', markersize=3, linewidth=1.2,\n",
        "                 label=f\"{r['tag']} (T={limit})\")\n",
        "    plt.xlabel('timestep')\n",
        "    plt.ylabel('mean distance error (1-step)')\n",
        "    max_label = 'all' if MAX_PLOT_STEPS is None else MAX_PLOT_STEPS\n",
        "    plt.title(f'1-step error per timestep (up to {max_label} or tag limit)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plot_path_step = output_root / 'distance_error_per_timestep.png'\n",
        "    plt.savefig(plot_path_step, dpi=150)\n",
        "    plt.show()\n",
        "    print('1ステップ誤差プロットを保存:', plot_path_step)\n",
        "\n",
        "    # -------- サマリ --------\n",
        "    print('\\nサマリ（平均距離誤差と長さ）')\n",
        "    for r in results:\n",
        "        err = np.array(r['distance_error_per_timestep'])\n",
        "        limit = resolve_limit(r['tag'], len(err))\n",
        "        err = err[:limit]\n",
        "        mean_full = float(err.mean())\n",
        "        mean_100 = float(err[:min(MAX_SUMMARY_STEPS, len(err))].mean())\n",
        "        print(f\"- {r['tag']}: steps={len(err)}, mean_full={mean_full:.6f}, mean@<=100={mean_100:.6f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}