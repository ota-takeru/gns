{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e989b71f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/kaggle/input/gns-codes に code も code.zip もありません。Add Data で gns-codes を追加してください。",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m     src = repo_dir\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/kaggle/input/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_SLUG\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m に code も code.zip もありません。Add Data で \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_SLUG\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m を追加してください。\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# フラット化（zip解凍で1階層挟まった場合）\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m src != repo_dir:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: /kaggle/input/gns-codes に code も code.zip もありません。Add Data で gns-codes を追加してください。"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Kaggle の入力データセット slug（username/slug の slug 部分）\n",
    "DATASET_SLUG = 'gns-codes'\n",
    "DATASET_ROOT = Path(f'/kaggle/input/{DATASET_SLUG}')\n",
    "WORK_ROOT = Path('/kaggle/working')\n",
    "repo_dir = WORK_ROOT / 'code'\n",
    "\n",
    "code_dir = DATASET_ROOT / 'code'\n",
    "code_zip = DATASET_ROOT / 'code.zip'\n",
    "\n",
    "# 展開済み code/ を優先し、無ければ code.zip を展開\n",
    "if code_dir.exists():\n",
    "    src = code_dir\n",
    "elif code_zip.exists():\n",
    "    if repo_dir.exists():\n",
    "        shutil.rmtree(repo_dir)\n",
    "    with zipfile.ZipFile(code_zip) as zf:\n",
    "        zf.extractall(repo_dir)\n",
    "    src = repo_dir\n",
    "else:\n",
    "    raise FileNotFoundError(f\"/kaggle/input/{DATASET_SLUG} に code も code.zip もありません。Add Data で {DATASET_SLUG} を追加してください。\")\n",
    "\n",
    "# フラット化（zip解凍で1階層挟まった場合）\n",
    "if src != repo_dir:\n",
    "    if repo_dir.exists():\n",
    "        shutil.rmtree(repo_dir)\n",
    "    shutil.copytree(src, repo_dir)\n",
    "if repo_dir.exists():\n",
    "    children = list(repo_dir.iterdir())\n",
    "    if len(children) == 1 and children[0].is_dir():\n",
    "        inner = children[0]\n",
    "        for p in inner.iterdir():\n",
    "            p.rename(repo_dir / p.name)\n",
    "        inner.rmdir()\n",
    "\n",
    "%cd $repo_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf889a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PyG の radius_graph で必要になる torch-cluster だけインストール\n",
    "import torch\n",
    "\n",
    "torch_ver = torch.__version__.split('+')[0]\n",
    "cuda_ver = torch.version.cuda\n",
    "cuda_tag = 'cpu' if cuda_ver is None else 'cu' + cuda_ver.replace('.', '')\n",
    "url = f\"https://data.pyg.org/whl/torch-{torch_ver}+{cuda_tag}.html\"\n",
    "print('PyG wheels:', url)\n",
    "\n",
    "!pip -q install torch-cluster -f {url}\n",
    "!pip -q install torch_geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語表示用パッケージのインストール\n",
    "!pip -q install japanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81493ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import copy\n",
    "import yaml\n",
    "import re\n",
    "import json\n",
    "\n",
    "REPO = Path('/kaggle/working/code')\n",
    "cfg_path = REPO / 'config_rollout.yaml'\n",
    "\n",
    "# 全モデルを同じデータセットで検証したい場合のスイッチ\n",
    "USE_FORCE_DATASET = True  # True にすると下記 FORCE_DATASET_ROOT を強制使用\n",
    "FORCE_DATASET_ROOT = Path('/kaggle/input/dam-break-left-800')\n",
    "\n",
    "# モデルごとに対応するデータセットを自動解決する\n",
    "\n",
    "def _load_metadata(path: Path):\n",
    "    try:\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _find_metadata(root: Path):\n",
    "    candidates = [\n",
    "        root / 'metadata.json',\n",
    "        root / 'metadata' / 'metadata.json',\n",
    "    ]\n",
    "    candidates += list(root.glob('**/metadata*.json'))\n",
    "    for c in candidates:\n",
    "        if c.is_file():\n",
    "            meta = _load_metadata(c)\n",
    "            if isinstance(meta, dict):\n",
    "                return meta, c\n",
    "    return None, None\n",
    "\n",
    "def _extract_sequence_length(meta: dict):\n",
    "    if not isinstance(meta, dict):\n",
    "        return None\n",
    "    for key in ('train', 'valid', 'test', 'rollout'):\n",
    "        part = meta.get(key)\n",
    "        if isinstance(part, dict) and 'sequence_length' in part:\n",
    "            try:\n",
    "                return int(part['sequence_length'])\n",
    "            except Exception:\n",
    "                pass\n",
    "    if 'sequence_length' in meta:\n",
    "        try:\n",
    "            return int(meta['sequence_length'])\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def resolve_dataset_root(step: int | None):\n",
    "    base = Path('/kaggle/input')\n",
    "    if not base.exists():\n",
    "        return None, False\n",
    "\n",
    "    if USE_FORCE_DATASET:\n",
    "        if FORCE_DATASET_ROOT.exists():\n",
    "            print(f\"[force] step={step} を無視して {FORCE_DATASET_ROOT} を使用します。\")\n",
    "            return FORCE_DATASET_ROOT, True\n",
    "        else:\n",
    "            print(f\"[force] {FORCE_DATASET_ROOT} が見つかりません。通常の解決ロジックに戻ります。\")\n",
    "\n",
    "    # 1) ディレクトリ名にステップ数が含まれるものを優先\n",
    "    if step is not None:\n",
    "        for dataset_root in sorted(base.iterdir()):\n",
    "            name = dataset_root.name\n",
    "            if (name.endswith(f'-{step}') or name.endswith(f'_{step}')\n",
    "                    or f'-{step}-' in name or f'_{step}_' in name):\n",
    "                return dataset_root, False\n",
    "\n",
    "    # 2) metadata の sequence_length で一致を探す\n",
    "    for dataset_root in sorted(base.iterdir()):\n",
    "        meta, meta_path = _find_metadata(dataset_root)\n",
    "        if not meta:\n",
    "            continue\n",
    "        seq = _extract_sequence_length(meta)\n",
    "        if step is not None and seq == step:\n",
    "            return dataset_root, False\n",
    "\n",
    "    # 3) フォールバック（もっとも汎用な800データセット）\n",
    "    fallback = base / 'dam-break-left-800'\n",
    "    if fallback.exists():\n",
    "        print(f\"[fallback] step={step} に一致するデータセットが見つからず、{fallback} を使用します。\")\n",
    "        return fallback, True\n",
    "    print(f\"[fallback] step={step} に一致するデータセットも既定の dam-break-left-800 も見つかりません。\")\n",
    "    return None, True\n",
    "\n",
    "# 出力設定（共通）\n",
    "output_root = REPO / 'rollouts'\n",
    "viz_format = 'html'  # html|mp4|gif\n",
    "\n",
    "with cfg_path.open('r', encoding='utf-8') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# データセットはモデルごとに後続セルで差し替える\n",
    "cfg['method'] = 'gns'\n",
    "cfg['rollout_inference_max_examples'] = None    # 検証データ全件で推論し平均を取る\n",
    "cfg['output_path'] = str(output_root)\n",
    "cfg.setdefault('scenario_options', {}).setdefault('fluid', {})['dataset'] = '/kaggle/input'  # placeholder\n",
    "\n",
    "\n",
    "def _get_nested(d, keys):\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return None\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "_raw_dt_candidates = [\n",
    "    _get_nested(cfg, ['scenario_options', 'fluid', 'integrator', 'dt']),\n",
    "    _get_nested(cfg, ['integrator', 'dt']),\n",
    "    _get_nested(cfg, ['method_options', 'hamiltonian_sph', 'integrator', 'dt']),\n",
    "]\n",
    "_raw_dt = next((v for v in _raw_dt_candidates if v is not None), 0.006)\n",
    "try:\n",
    "    INFER_DT = float(_raw_dt)\n",
    "    if INFER_DT <= 0:\n",
    "        raise ValueError\n",
    "except Exception:\n",
    "    INFER_DT = 0.006\n",
    "\n",
    "# モデルは後続セルで差し替える（ベースを保存しておく）\n",
    "cfg['model_path'] = str(REPO / 'models')\n",
    "cfg['model_file'] = None\n",
    "cfg['output_filename'] = 'rollout'\n",
    "\n",
    "BASE_CFG = copy.deepcopy(cfg)\n",
    "\n",
    "with cfg_path.open('w', encoding='utf-8') as f:\n",
    "    yaml.safe_dump(cfg, f, allow_unicode=True)\n",
    "\n",
    "print('ベース設定を書き出しました:', cfg_path)\n",
    "print('output_path:', cfg['output_path'])\n",
    "print('inference dt:', INFER_DT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/code\n",
    "\n",
    "import copy\n",
    "import re\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(str(Path('/kaggle/working/code/src')))\n",
    "\n",
    "from analyze_rollouts import analyze_rollout\n",
    "from simulator_factory import _get_simulator\n",
    "from train_config import load_config, INPUT_SEQUENCE_LENGTH, KINEMATIC_PARTICLE_ID\n",
    "from train_paths import _resolve_model_path\n",
    "from train_utils import _resolve_rollout_dataset_path\n",
    "import data_loader\n",
    "import reading_utils\n",
    "\n",
    "# 比較対象モデルを集約（rollout_diff 優先）\n",
    "model_roots = [\n",
    "    REPO / 'models' / 'dataset_diff',\n",
    "    REPO / 'models',\n",
    "]\n",
    "model_files: list[Path] = []\n",
    "for root in model_roots:\n",
    "    if not root.exists():\n",
    "        continue\n",
    "    for path in root.glob('*.pt'):\n",
    "        model_files.append(path.resolve())\n",
    "\n",
    "# 重複除去 & ソート（親ディレクトリ優先→ステップ番号）\n",
    "seen = set()\n",
    "unique_models: list[Path] = []\n",
    "for p in model_files:\n",
    "    if p in seen:\n",
    "        continue\n",
    "    seen.add(p)\n",
    "    unique_models.append(p)\n",
    "\n",
    "def _step_key(p: Path):\n",
    "    m = re.search(r'(\\d+)', p.stem)\n",
    "    step = int(m.group(1)) if m else -1\n",
    "    priority = 0 if p.parent.name == \"dataset_diff\" else 1\n",
    "    return (priority, p.parent.name, step)\n",
    "\n",
    "def _step_from_path(p: Path):\n",
    "    m = re.search(r'(\\d+)(?!.*\\d)', p.stem)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "model_files = sorted(unique_models, key=_step_key)\n",
    "\n",
    "if not model_files:\n",
    "    raise FileNotFoundError('*.pt が見つかりません。models/ 下にモデルを配置してください。')\n",
    "\n",
    "print('評価対象モデル:')\n",
    "for p in model_files:\n",
    "    print(' -', p)\n",
    "\n",
    "\n",
    "# キャッシュ読み込み/保存用\n",
    "cache_path = output_root / 'inference_results.json'\n",
    "results = []\n",
    "\n",
    "if cache_path.exists():\n",
    "    with cache_path.open('r', encoding='utf-8') as f:\n",
    "        saved = json.load(f)\n",
    "    results = saved.get('results', [])\n",
    "    print(f\"キャッシュを読み込みました: {cache_path}\")\n",
    "    print(f\"キャッシュ内のモデル数: {len(results)}\")\n",
    "else:\n",
    "    def compute_one_step_error(cfg_path: Path, device: torch.device, dt: float):\n",
    "        # 検証データ全軌跡で1-step誤差を平均\n",
    "        cfg_obj = load_config(str(cfg_path))\n",
    "        dataset_path = _resolve_rollout_dataset_path(cfg_obj)\n",
    "        if dataset_path is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"rollout/valid/test/train の npz が見つかりません: {cfg_obj.data_path}\"\n",
    "            )\n",
    "\n",
    "        metadata_key = (\n",
    "            cfg_obj.active_scenario.rollout_metadata_split\n",
    "            if getattr(cfg_obj, 'active_scenario', None)\n",
    "            and cfg_obj.active_scenario.rollout_metadata_split\n",
    "            else 'rollout'\n",
    "        )\n",
    "        metadata = reading_utils.read_metadata(cfg_obj.data_path, metadata_key)\n",
    "        if isinstance(metadata, dict) and 'acc_mean' not in metadata:\n",
    "            metadata = metadata.get('rollout') or metadata.get('train') or metadata\n",
    "        simulator = _get_simulator(\n",
    "            metadata,\n",
    "            cfg_obj.noise_std,\n",
    "            cfg_obj.noise_std,\n",
    "            device,\n",
    "            cfg_obj,\n",
    "        )\n",
    "        model_file = _resolve_model_path(cfg_obj)\n",
    "        simulator.load(model_file)\n",
    "        simulator.to(device)\n",
    "        simulator.eval()\n",
    "\n",
    "        loader = data_loader.get_data_loader_by_trajectories(dataset_path)\n",
    "\n",
    "        step_sums = None\n",
    "        vel_sums = None\n",
    "        acc_sums = None\n",
    "        step_counts = None\n",
    "        n_rollouts = 0\n",
    "\n",
    "        dt = float(dt) if dt and dt > 0 else 1.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for traj in loader:\n",
    "                if len(traj) == 4:\n",
    "                    positions, particle_type, material_property, n_particles = traj\n",
    "                    material_property = material_property.to(device)\n",
    "                else:\n",
    "                    positions, particle_type, n_particles = traj\n",
    "                    material_property = None\n",
    "\n",
    "                positions = positions.to(device)\n",
    "                particle_type = particle_type.to(device)\n",
    "                n_particles_tensor = torch.tensor(\n",
    "                    [int(n_particles)], device=device, dtype=torch.int32\n",
    "                )\n",
    "\n",
    "                total_steps_in_traj = positions.shape[1] - INPUT_SEQUENCE_LENGTH\n",
    "                if total_steps_in_traj <= 0:\n",
    "                    continue\n",
    "\n",
    "                step_errors: list[float] = []\n",
    "                vel_errors: list[float] = []\n",
    "                acc_errors: list[float] = []\n",
    "                for t in range(total_steps_in_traj):\n",
    "                    window = positions[:, t : t + INPUT_SEQUENCE_LENGTH]\n",
    "                    target = positions[:, t + INPUT_SEQUENCE_LENGTH]\n",
    "\n",
    "                    pred = simulator.predict_positions(\n",
    "                        window,\n",
    "                        nparticles_per_example=n_particles_tensor,\n",
    "                        particle_types=particle_type,\n",
    "                        material_property=material_property,\n",
    "                    )\n",
    "\n",
    "                    # 運動制約粒子は GT を強制\n",
    "                    kinematic_mask = (particle_type == KINEMATIC_PARTICLE_ID).to(device)\n",
    "                    pred = torch.where(kinematic_mask[:, None], target, pred)\n",
    "\n",
    "                    dist = torch.sqrt(((pred - target) ** 2).sum(dim=-1))\n",
    "                    vel_pred = (pred - window[:, -1]) / dt\n",
    "                    vel_target = (target - window[:, -1]) / dt\n",
    "                    vel_dist = torch.sqrt(((vel_pred - vel_target) ** 2).sum(dim=-1))\n",
    "\n",
    "                    # 加速度は2階差分\n",
    "                    accel_pred = (pred - 2 * window[:, -1] + window[:, -2]) / (dt**2)\n",
    "                    accel_target = (target - 2 * window[:, -1] + window[:, -2]) / (dt**2)\n",
    "                    accel_dist = torch.sqrt(((accel_pred - accel_target) ** 2).sum(dim=-1))\n",
    "\n",
    "                    step_errors.append(float(dist.mean().item()))\n",
    "                    vel_errors.append(float(vel_dist.mean().item()))\n",
    "                    acc_errors.append(float(accel_dist.mean().item()))\n",
    "\n",
    "                if step_errors:\n",
    "                    n_rollouts += 1\n",
    "                    if step_sums is None:\n",
    "                        step_sums = np.zeros(len(step_errors), dtype=float)\n",
    "                        vel_sums = np.zeros(len(step_errors), dtype=float)\n",
    "                        acc_sums = np.zeros(len(step_errors), dtype=float)\n",
    "                        step_counts = np.zeros(len(step_errors), dtype=int)\n",
    "                    elif len(step_errors) > len(step_sums):\n",
    "                        # 可変長シーケンスに対応してパディング\n",
    "                        pad_len = len(step_errors) - len(step_sums)\n",
    "                        step_sums = np.pad(step_sums, (0, pad_len))\n",
    "                        vel_sums = np.pad(vel_sums, (0, pad_len))\n",
    "                        acc_sums = np.pad(acc_sums, (0, pad_len))\n",
    "                        step_counts = np.pad(step_counts, (0, pad_len))\n",
    "\n",
    "                    step_sums[: len(step_errors)] += step_errors\n",
    "                    vel_sums[: len(step_errors)] += vel_errors\n",
    "                    acc_sums[: len(step_errors)] += acc_errors\n",
    "                    step_counts[: len(step_errors)] += 1\n",
    "\n",
    "        if not n_rollouts or step_sums is None or vel_sums is None or acc_sums is None:\n",
    "            return None\n",
    "\n",
    "        valid_mask = step_counts > 0\n",
    "        mean_per_timestep = (step_sums[valid_mask] / step_counts[valid_mask]).tolist()\n",
    "        mean_vel_per_timestep = (vel_sums[valid_mask] / step_counts[valid_mask]).tolist()\n",
    "        mean_acc_per_timestep = (acc_sums[valid_mask] / step_counts[valid_mask]).tolist()\n",
    "        mean_distance_error = float(step_sums.sum() / step_counts.sum())\n",
    "        mean_velocity_error = float(vel_sums.sum() / step_counts.sum())\n",
    "        mean_acceleration_error = float(acc_sums.sum() / step_counts.sum())\n",
    "        used_length = int(valid_mask.sum())\n",
    "\n",
    "        return {\n",
    "            'per_timestep': mean_per_timestep,\n",
    "            'velocity_per_timestep': mean_vel_per_timestep,\n",
    "            'acceleration_per_timestep': mean_acc_per_timestep,\n",
    "            'mean_distance_error': mean_distance_error,\n",
    "            'mean_velocity_error': mean_velocity_error,\n",
    "            'mean_acceleration_error': mean_acceleration_error,\n",
    "            'n_rollouts': n_rollouts,\n",
    "            'used_length': used_length,\n",
    "        }\n",
    "\n",
    "\n",
    "    cmd = ['python', 'src/train.py', '--config', str(cfg_path)]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    for model_path in model_files:\n",
    "        tag = model_path.stem  # ディレクトリ名を含めずモデル名のみ\n",
    "        step_num = _step_from_path(model_path)\n",
    "        dataset_root, used_fallback = resolve_dataset_root(step_num)\n",
    "        if dataset_root is None:\n",
    "            raise FileNotFoundError(f\"対応するデータセットが見つかりません（step={step_num}）\")\n",
    "\n",
    "        cfg = copy.deepcopy(BASE_CFG)\n",
    "        cfg['model_path'] = str(model_path.parent)\n",
    "        cfg['model_file'] = str(model_path)\n",
    "        cfg['output_filename'] = f\"rollout_{tag}\"\n",
    "        cfg.setdefault('scenario_options', {}).setdefault('fluid', {})['dataset'] = str(dataset_root)\n",
    "        cfg['data_path'] = str(dataset_root)\n",
    "        cfg['rollout_dataset'] = 'valid'\n",
    "\n",
    "        with cfg_path.open('w', encoding='utf-8') as f:\n",
    "            yaml.safe_dump(cfg, f, allow_unicode=True)\n",
    "\n",
    "        print(f\"=== {tag} ===\")\n",
    "        print('dataset:', dataset_root)\n",
    "        if used_fallback:\n",
    "            print('[warn] 指定ステップに一致するデータセットが見つからなかったためフォールバックを使用しました。')\n",
    "        print('Running:', ' '.join(cmd))\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "        output_dir = Path(cfg['output_path']) / cfg['method'] / cfg['output_filename']\n",
    "        pkl_files = sorted(output_dir.glob(f\"{cfg['output_filename']}_ex*.pkl\"))\n",
    "        if not pkl_files:\n",
    "            raise FileNotFoundError(f\"{output_dir} に *_ex*.pkl がありません。推論が成功したか確認してください。\")\n",
    "\n",
    "        rollout_dists = []\n",
    "        rollout_vels = []\n",
    "        rollout_accs = []\n",
    "        for pkl_file in pkl_files:\n",
    "            res_one = analyze_rollout(pkl_file, dt=INFER_DT)\n",
    "            dist = np.array(res_one['distance_error_per_timestep'])\n",
    "            rollout_dists.append(dist)\n",
    "            vel_raw = res_one.get('velocity_error_per_timestep')\n",
    "            vel = np.array(vel_raw) if vel_raw is not None else np.array([])\n",
    "            if vel.size:\n",
    "                rollout_vels.append(vel)\n",
    "            acc_raw = res_one.get('acceleration_error_per_timestep')\n",
    "            acc = np.array(acc_raw) if acc_raw is not None else np.array([])\n",
    "            if acc.size:\n",
    "                rollout_accs.append(acc)\n",
    "\n",
    "        max_len = max(len(d) for d in rollout_dists)\n",
    "        dist_sums = np.zeros(max_len)\n",
    "        dist_counts = np.zeros(max_len, dtype=int)\n",
    "        for dist in rollout_dists:\n",
    "            dist_sums[: len(dist)] += dist\n",
    "            dist_counts[: len(dist)] += 1\n",
    "\n",
    "        valid_mask = dist_counts > 0\n",
    "        mean_dist_per_timestep = (dist_sums[valid_mask] / dist_counts[valid_mask]).tolist()\n",
    "        weighted_mean = float(dist_sums.sum() / dist_counts.sum())\n",
    "        used_length = int(valid_mask.sum())\n",
    "        avg_steps = float(sum(len(d) for d in rollout_dists) / len(rollout_dists))\n",
    "\n",
    "        if rollout_vels:\n",
    "            max_len_vel = max(len(v) for v in rollout_vels)\n",
    "            vel_sums = np.zeros(max_len_vel)\n",
    "            vel_counts = np.zeros(max_len_vel, dtype=int)\n",
    "            for vel in rollout_vels:\n",
    "                vel_sums[: len(vel)] += vel\n",
    "                vel_counts[: len(vel)] += 1\n",
    "            valid_mask_vel = vel_counts > 0\n",
    "            mean_vel_per_timestep = (vel_sums[valid_mask_vel] / vel_counts[valid_mask_vel]).tolist()\n",
    "            weighted_mean_vel = float(vel_sums.sum() / vel_counts.sum())\n",
    "            used_length_vel = int(valid_mask_vel.sum())\n",
    "        else:\n",
    "            mean_vel_per_timestep = []\n",
    "            weighted_mean_vel = None\n",
    "            used_length_vel = 0\n",
    "\n",
    "        if rollout_accs:\n",
    "            max_len_acc = max(len(a) for a in rollout_accs)\n",
    "            acc_sums = np.zeros(max_len_acc)\n",
    "            acc_counts = np.zeros(max_len_acc, dtype=int)\n",
    "            for acc in rollout_accs:\n",
    "                acc_sums[: len(acc)] += acc\n",
    "                acc_counts[: len(acc)] += 1\n",
    "            valid_mask_acc = acc_counts > 0\n",
    "            mean_acc_per_timestep = (acc_sums[valid_mask_acc] / acc_counts[valid_mask_acc]).tolist()\n",
    "            weighted_mean_acc = float(acc_sums.sum() / acc_counts.sum())\n",
    "            used_length_acc = int(valid_mask_acc.sum())\n",
    "        else:\n",
    "            mean_acc_per_timestep = []\n",
    "            weighted_mean_acc = None\n",
    "            used_length_acc = 0\n",
    "\n",
    "        one_step_res = compute_one_step_error(cfg_path, device, dt=INFER_DT)\n",
    "        if one_step_res:\n",
    "            print(\n",
    "                f\"  one-step: rollouts={one_step_res['n_rollouts']} / mean distance error={one_step_res['mean_distance_error']:.6f} / mean velocity error={one_step_res['mean_velocity_error']:.6f} / mean acc error={one_step_res['mean_acceleration_error']:.6f}\"\n",
    "            )\n",
    "        else:\n",
    "            print('  one-step: 計算できませんでした。')\n",
    "\n",
    "        vel_msg = f\"{weighted_mean_vel:.6f}\" if weighted_mean_vel is not None else 'N/A'\n",
    "        acc_msg = f\"{weighted_mean_acc:.6f}\" if weighted_mean_acc is not None else 'N/A'\n",
    "        results.append({\n",
    "            'tag': tag,\n",
    "            'distance_error_per_timestep': mean_dist_per_timestep,\n",
    "            'mean_distance_error': weighted_mean,\n",
    "            'n_rollouts': len(rollout_dists),\n",
    "            'used_length': used_length,\n",
    "            'pkl_dir': str(output_dir),\n",
    "            'dataset_root': str(dataset_root),\n",
    "            'step_num': step_num,\n",
    "            'used_fallback': used_fallback,\n",
    "            'velocity_error_per_timestep': mean_vel_per_timestep,\n",
    "            'mean_velocity_error': weighted_mean_vel,\n",
    "            'velocity_used_length': used_length_vel,\n",
    "            'acceleration_error_per_timestep': mean_acc_per_timestep,\n",
    "            'mean_acceleration_error': weighted_mean_acc,\n",
    "            'acceleration_used_length': used_length_acc,\n",
    "            'one_step_error_per_timestep': one_step_res['per_timestep'] if one_step_res else [],\n",
    "            'one_step_mean_distance_error': one_step_res['mean_distance_error'] if one_step_res else None,\n",
    "            'one_step_velocity_error_per_timestep': one_step_res['velocity_per_timestep'] if one_step_res else [],\n",
    "            'one_step_mean_velocity_error': one_step_res['mean_velocity_error'] if one_step_res else None,\n",
    "            'one_step_acceleration_error_per_timestep': one_step_res['acceleration_per_timestep'] if one_step_res else [],\n",
    "            'one_step_mean_acceleration_error': one_step_res['mean_acceleration_error'] if one_step_res else None,\n",
    "            'one_step_used_length': one_step_res['used_length'] if one_step_res else 0,\n",
    "            'one_step_n_rollouts': one_step_res['n_rollouts'] if one_step_res else 0,\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"  rollout: rollouts={len(rollout_dists)} / avg steps per rollout={avg_steps:.1f} / mean distance error={weighted_mean:.6f} / mean velocity error={vel_msg} / mean acc error={acc_msg} (valid 平均)\"\n",
    "        )\n",
    "\n",
    "    # 推論結果をキャッシュ保存\n",
    "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with cache_path.open('w', encoding='utf-8') as f:\n",
    "        json.dump({'results': results, 'infer_dt': INFER_DT}, f, ensure_ascii=False, indent=2)\n",
    "    print('結果をキャッシュに保存しました:', cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import cycle\n",
    "\n",
    "# 日本語表示設定\n",
    "try:\n",
    "    import japanize_matplotlib\n",
    "except ImportError:\n",
    "    plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'DejaVu Sans', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "MAX_SUMMARY_STEPS = 100\n",
    "# None なら上限なし\n",
    "MAX_PLOT_STEPS = None  # 1ステップ誤差の表示上限\n",
    "\n",
    "# ラベル・凡例のサイズ設定（大きめ）\n",
    "LABEL_FONTSIZE = 16\n",
    "TICK_FONTSIZE = 14\n",
    "LEGEND_FONTSIZE = 16\n",
    "TITLE_FONTSIZE = 18\n",
    "\n",
    "if not results:\n",
    "    print('results が空です。前のセルを実行してください。')\n",
    "else:\n",
    "    # モデルごとに固定の色を割り当て（rollout と 1step で揃える）\n",
    "    color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get('color', [])\n",
    "    if not color_cycle:\n",
    "        color_cycle = list(plt.cm.tab10.colors)\n",
    "    color_iter = cycle(color_cycle)\n",
    "    color_map = {r['tag']: next(color_iter) for r in results}\n",
    "\n",
    "    # -------- 全タイムステップの距離誤差（rollout 平均） --------\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    max_len_rollout = max(len(np.array(r['distance_error_per_timestep'])) for r in results)\n",
    "    for r in results:\n",
    "        err = np.array(r['distance_error_per_timestep'])\n",
    "        timesteps = np.arange(len(err))\n",
    "        # 短い系列ほど zorder を高くして上に重ねる\n",
    "        z = max_len_rollout - len(err)\n",
    "        plt.plot(\n",
    "            timesteps,\n",
    "            err,\n",
    "            label=f\"{r['tag']} (L={len(err)})\",\n",
    "            color=color_map[r['tag']],\n",
    "            zorder=z,\n",
    "        )\n",
    "    plt.xlabel('タイムステップ', fontsize=LABEL_FONTSIZE)\n",
    "    plt.ylabel(\"ロールアウト誤差(20サンプル平均)\", fontsize=LABEL_FONTSIZE)\n",
    "    plt.title('タイムステップ数を変えたときのロールアウト誤差）', fontsize=TITLE_FONTSIZE)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tick_params(labelsize=TICK_FONTSIZE)\n",
    "    plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = output_root / 'distance_error_comparison.png'\n",
    "    plt.savefig(plot_path, dpi=150)\n",
    "    plt.show()\n",
    "    print('プロットを保存:', plot_path)\n",
    "\n",
    "    # -------- 全タイムステップの速度誤差（rollout 平均） --------\n",
    "    vel_plot_items = []\n",
    "    for r in results:\n",
    "        err = np.array(r.get('velocity_error_per_timestep') or [])\n",
    "        if err.size == 0:\n",
    "            continue\n",
    "        vel_plot_items.append((len(err), err, r))\n",
    "    if vel_plot_items:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        max_len_rollout_vel = max(item[0] for item in vel_plot_items)\n",
    "        for _, err, r in vel_plot_items:\n",
    "            timesteps = np.arange(len(err))\n",
    "            z = max_len_rollout_vel - len(err)\n",
    "            plt.plot(\n",
    "                timesteps,\n",
    "                err,\n",
    "                label=f\"{r['tag']} (L={len(err)})\",\n",
    "                color=color_map[r['tag']],\n",
    "                zorder=z,\n",
    "            )\n",
    "        plt.xlabel('タイムステップ', fontsize=LABEL_FONTSIZE)\n",
    "        plt.ylabel('1ステップ誤差', fontsize=LABEL_FONTSIZE)\n",
    "        plt.title(\"タイムステップ数を変えたときの1ステップ誤差\", fontsize=TITLE_FONTSIZE)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tick_params(labelsize=TICK_FONTSIZE)\n",
    "        plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_path_vel = output_root / 'velocity_error_comparison.png'\n",
    "        plt.savefig(plot_path_vel, dpi=150)\n",
    "        plt.show()\n",
    "        print('速度誤差プロットを保存:', plot_path_vel)\n",
    "\n",
    "    # -------- 全タイムステップの加速度誤差（rollout 平均） --------\n",
    "    acc_plot_items = []\n",
    "    for r in results:\n",
    "        err = np.array(r.get('acceleration_error_per_timestep') or [])\n",
    "        if err.size == 0:\n",
    "            continue\n",
    "        acc_plot_items.append((len(err), err, r))\n",
    "    if acc_plot_items:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        max_len_rollout_acc = max(item[0] for item in acc_plot_items)\n",
    "        for _, err, r in acc_plot_items:\n",
    "            timesteps = np.arange(len(err))\n",
    "            z = max_len_rollout_acc - len(err)\n",
    "            plt.plot(\n",
    "                timesteps,\n",
    "                err,\n",
    "                label=f\"{r['tag']} (L={len(err)}, ロールアウト本数={r['n_rollouts']})\",\n",
    "                color=color_map[r['tag']],\n",
    "                zorder=z,\n",
    "            )\n",
    "        plt.xlabel('タイムステップ', fontsize=LABEL_FONTSIZE)\n",
    "        plt.ylabel('加速度誤差の平均（検証データ平均）', fontsize=LABEL_FONTSIZE)\n",
    "        plt.title('ロールアウト加速度誤差（タイムステップ別、検証平均）', fontsize=TITLE_FONTSIZE)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tick_params(labelsize=TICK_FONTSIZE)\n",
    "        plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_path_acc = output_root / 'acceleration_error_comparison.png'\n",
    "        plt.savefig(plot_path_acc, dpi=150)\n",
    "        plt.show()\n",
    "        print('加速度誤差プロットを保存:', plot_path_acc)\n",
    "\n",
    "    # -------- 1ステップ誤差（教師あり1-step, validation 平均） --------\n",
    "    plot_items = []\n",
    "    for r in results:\n",
    "        err = np.array(r.get('one_step_error_per_timestep') or [])\n",
    "        if err.size == 0:\n",
    "            continue\n",
    "        if MAX_PLOT_STEPS is not None:\n",
    "            err = err[:MAX_PLOT_STEPS]\n",
    "        # results の順序を維持して rollouts と同じ並びにする\n",
    "        plot_items.append((len(err), err, r))\n",
    "\n",
    "    if plot_items:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        max_len_onestep = max(len(item[1]) for item in plot_items)\n",
    "        for _, err, r in plot_items:\n",
    "            timesteps = np.arange(len(err))\n",
    "            z = max_len_onestep - len(err)\n",
    "            plt.plot(\n",
    "                timesteps,\n",
    "                err,\n",
    "                marker='o',\n",
    "                markersize=3,\n",
    "                linewidth=1.2,\n",
    "                label=f\"{r['tag']} (L={len(err)}, 1ステップ本数={r['one_step_n_rollouts']})\",\n",
    "                color=color_map[r['tag']],\n",
    "                zorder=z,\n",
    "            )\n",
    "        plt.xlabel('タイムステップ', fontsize=LABEL_FONTSIZE)\n",
    "        plt.ylabel('距離誤差の平均（1ステップ、検証平均）', fontsize=LABEL_FONTSIZE)\n",
    "        plt.title('1ステップ距離誤差（タイムステップ別、検証平均）', fontsize=TITLE_FONTSIZE)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tick_params(labelsize=TICK_FONTSIZE)\n",
    "        plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_path_step = output_root / 'distance_error_per_timestep.png'\n",
    "        plt.savefig(plot_path_step, dpi=150)\n",
    "        plt.show()\n",
    "        print('1ステップ距離誤差プロットを保存:', plot_path_step)\n",
    "\n",
    "    # -------- 1ステップの速度誤差（validation 平均） --------\n",
    "    vel_step_items = []\n",
    "    for r in results:\n",
    "        err = np.array(r.get('one_step_velocity_error_per_timestep') or [])\n",
    "        if err.size == 0:\n",
    "            continue\n",
    "        if MAX_PLOT_STEPS is not None:\n",
    "            err = err[:MAX_PLOT_STEPS]\n",
    "        vel_step_items.append((len(err), err, r))\n",
    "\n",
    "    if vel_step_items:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        max_len_onestep_vel = max(len(item[1]) for item in vel_step_items)\n",
    "        for _, err, r in vel_step_items:\n",
    "            timesteps = np.arange(len(err))\n",
    "            z = max_len_onestep_vel - len(err)\n",
    "            plt.plot(\n",
    "                timesteps,\n",
    "                err,\n",
    "                marker='o',\n",
    "                markersize=3,\n",
    "                linewidth=1.2,\n",
    "                label=f\"{r['tag']} (L={len(err)}, 1ステップ本数={r['one_step_n_rollouts']})\",\n",
    "                color=color_map[r['tag']],\n",
    "                zorder=z,\n",
    "            )\n",
    "        plt.xlabel('タイムステップ', fontsize=LABEL_FONTSIZE)\n",
    "        plt.ylabel('速度誤差の平均（1ステップ、検証平均）', fontsize=LABEL_FONTSIZE)\n",
    "        plt.title('1ステップ速度誤差（タイムステップ別、検証平均）', fontsize=TITLE_FONTSIZE)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tick_params(labelsize=TICK_FONTSIZE)\n",
    "        plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_path_vel_step = output_root / 'velocity_error_per_timestep.png'\n",
    "        plt.savefig(plot_path_vel_step, dpi=150)\n",
    "        plt.show()\n",
    "        print('1ステップ速度誤差プロットを保存:', plot_path_vel_step)\n",
    "\n",
    "    # -------- 1ステップの加速度誤差（validation 平均） --------\n",
    "    acc_step_items = []\n",
    "    for r in results:\n",
    "        err = np.array(r.get('one_step_acceleration_error_per_timestep') or [])\n",
    "        if err.size == 0:\n",
    "            continue\n",
    "        if MAX_PLOT_STEPS is not None:\n",
    "            err = err[:MAX_PLOT_STEPS]\n",
    "        acc_step_items.append((len(err), err, r))\n",
    "\n",
    "    if acc_step_items:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        max_len_onestep_acc = max(len(item[1]) for item in acc_step_items)\n",
    "        for _, err, r in acc_step_items:\n",
    "            timesteps = np.arange(len(err))\n",
    "            z = max_len_onestep_acc - len(err)\n",
    "            plt.plot(\n",
    "                timesteps,\n",
    "                err,\n",
    "                marker='o',\n",
    "                markersize=3,\n",
    "                linewidth=1.2,\n",
    "                label=f\"{r['tag']} (L={len(err)}, 1ステップ本数={r['one_step_n_rollouts']})\",\n",
    "                color=color_map[r['tag']],\n",
    "                zorder=z,\n",
    "            )\n",
    "        plt.xlabel('タイムステップ', fontsize=LABEL_FONTSIZE)\n",
    "        plt.ylabel('加速度誤差の平均（1ステップ、検証平均）', fontsize=LABEL_FONTSIZE)\n",
    "        plt.title('1ステップ加速度誤差（タイムステップ別、検証平均）', fontsize=TITLE_FONTSIZE)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tick_params(labelsize=TICK_FONTSIZE)\n",
    "        plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_path_acc_step = output_root / 'acceleration_error_per_timestep.png'\n",
    "        plt.savefig(plot_path_acc_step, dpi=150)\n",
    "        plt.show()\n",
    "        print('1ステップ加速度誤差プロットを保存:', plot_path_acc_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# 日本語表示設定\n",
    "try:\n",
    "    import japanize_matplotlib\n",
    "except ImportError:\n",
    "    plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'DejaVu Sans', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "DT = INFER_DT  # 推論と同じ刻み幅を使用\n",
    "target_name = 'dam-break-left-800'\n",
    "\n",
    "if not results:\n",
    "    print('results が空です。前のセルを実行してください。')\n",
    "else:\n",
    "    target = [r for r in results if target_name in str(r.get('dataset_root', ''))]\n",
    "    if not target:\n",
    "        print(f\"{target_name} を含むデータセットの結果が見つかりません。先に FORCE を有効化するか、対象モデルだけ実行してください。\")\n",
    "    else:\n",
    "        color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get('color', []) or list(plt.cm.tab10.colors)\n",
    "        color_map = {r['tag']: color_cycle[i % len(color_cycle)] for i, r in enumerate(target)}\n",
    "\n",
    "        def _load_rms(pkl_path: Path):\n",
    "            with pkl_path.open('rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            pred = data['predicted_rollout']\n",
    "            gt = data['ground_truth_rollout']\n",
    "            if pred.ndim == 4:\n",
    "                pred = pred[:, 0]\n",
    "            if gt.ndim == 4:\n",
    "                gt = gt[:, 0]\n",
    "            v_pred = np.diff(pred, axis=0) / DT\n",
    "            v_gt = np.diff(gt, axis=0) / DT\n",
    "            a_pred = np.diff(v_pred, axis=0) / DT\n",
    "            a_gt = np.diff(v_gt, axis=0) / DT\n",
    "            v_rms_pred = np.sqrt((v_pred ** 2).mean(axis=(1, 2)))\n",
    "            v_rms_gt = np.sqrt((v_gt ** 2).mean(axis=(1, 2)))\n",
    "            a_rms_pred = np.sqrt((a_pred ** 2).mean(axis=(1, 2)))\n",
    "            a_rms_gt = np.sqrt((a_gt ** 2).mean(axis=(1, 2)))\n",
    "            return v_rms_pred, v_rms_gt, a_rms_pred, a_rms_gt\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "        for r in target:\n",
    "            pdir = Path(r['pkl_dir'])\n",
    "            pkls = sorted(pdir.glob('*_ex*.pkl'))\n",
    "            if not pkls:\n",
    "                print('pkl が見つかりません:', pdir)\n",
    "                continue\n",
    "            v_pred, v_gt, a_pred, a_gt = _load_rms(pkls[0])\n",
    "            t_v = np.arange(len(v_pred))\n",
    "            t_a = np.arange(len(a_pred))\n",
    "            color = color_map[r['tag']]\n",
    "            axes[0].plot(t_v, v_gt, linestyle='--', color=color, alpha=0.8, label=f\"{r['tag']} 正解\")\n",
    "            axes[0].plot(t_v, v_pred, linestyle='-', color=color, label=f\"{r['tag']} 予測\")\n",
    "            axes[1].plot(t_a, a_gt, linestyle='--', color=color, alpha=0.8, label=f\"{r['tag']} 正解\")\n",
    "            axes[1].plot(t_a, a_pred, linestyle='-', color=color, label=f\"{r['tag']} 予測\")\n",
    "\n",
    "        axes[0].set_ylabel('速度RMS')\n",
    "        axes[0].set_title(f'速度RMSの時間変化)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[1].set_xlabel('タイムステップ')\n",
    "        axes[1].set_ylabel('加速度RMS')\n",
    "        axes[1].set_title(f'加速度RMSの時間変化')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        handles, labels = axes[0].get_legend_handles_labels()\n",
    "        axes[0].legend(handles, labels, loc='upper right')\n",
    "        handles, labels = axes[1].get_legend_handles_labels()\n",
    "        axes[1].legend(handles, labels, loc='upper right')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        out_path = output_root / f'rms_{target_name}.png'\n",
    "        fig.savefig(out_path, dpi=150)\n",
    "        plt.show()\n",
    "        print('RMS プロットを保存:', out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 日本語表示設定\n",
    "try:\n",
    "    import japanize_matplotlib\n",
    "except ImportError:\n",
    "    plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'DejaVu Sans', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "TARGET_STEP = 800\n",
    "MA_WINDOW = 50  # 移動平均の窓幅（低周波成分用）\n",
    "\n",
    "def moving_average(arr: np.ndarray, window: int) -> np.ndarray:\n",
    "    if arr.size == 0 or window <= 1:\n",
    "        return arr.astype(float)\n",
    "    window = min(window, len(arr))\n",
    "    kernel = np.ones(window, dtype=float) / window\n",
    "    return np.convolve(arr, kernel, mode='same')\n",
    "\n",
    "def split_low_high(arr: np.ndarray, window: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    low = moving_average(arr, window)\n",
    "    high = arr.astype(float) - low\n",
    "    return low, high\n",
    "\n",
    "def rms(x: np.ndarray) -> float:\n",
    "    if x.size == 0:\n",
    "        return float('nan')\n",
    "    return float(np.sqrt(np.mean(x ** 2)))\n",
    "\n",
    "if not results:\n",
    "    print('results が空です。前のセルを実行してください。')\n",
    "else:\n",
    "    target_models = [r for r in results if r.get('step_num') == TARGET_STEP]\n",
    "    if not target_models:\n",
    "        print(f\"{TARGET_STEP} ステップのモデルが見つかりません。step_num を確認してください。\")\n",
    "    else:\n",
    "        color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get('color', []) or list(plt.cm.tab10.colors)\n",
    "        color_map = {r['tag']: color_cycle[i % len(color_cycle)] for i, r in enumerate(target_models)}\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=False)\n",
    "        summary = []\n",
    "\n",
    "        for r in target_models:\n",
    "            tag = r['tag']\n",
    "            vel = np.array(r.get('one_step_velocity_error_per_timestep') or [])\n",
    "            acc = np.array(r.get('one_step_acceleration_error_per_timestep') or [])\n",
    "\n",
    "            if vel.size:\n",
    "                low_v, high_v = split_low_high(vel, MA_WINDOW)\n",
    "                t_v = np.arange(len(vel))\n",
    "                axes[0].plot(t_v, low_v, color=color_map[tag], label=f\"{tag} 低周波\")\n",
    "                axes[0].plot(t_v, high_v, color=color_map[tag], linestyle='--', alpha=0.7, label=f\"{tag} 高周波\")\n",
    "                summary.append(f\"{tag} 速度: 低周波RMS={rms(low_v):.6f}, 高周波RMS={rms(high_v):.6f}\")\n",
    "            else:\n",
    "                summary.append(f\"{tag} 速度: データなし\")\n",
    "\n",
    "            if acc.size:\n",
    "                low_a, high_a = split_low_high(acc, MA_WINDOW)\n",
    "                t_a = np.arange(len(acc))\n",
    "                axes[1].plot(t_a, low_a, color=color_map[tag], label=f\"{tag} 低周波\")\n",
    "                axes[1].plot(t_a, high_a, color=color_map[tag], linestyle='--', alpha=0.7, label=f\"{tag} 高周波\")\n",
    "                summary.append(f\"{tag} 加速度: 低周波RMS={rms(low_a):.6f}, 高周波RMS={rms(high_a):.6f}\")\n",
    "            else:\n",
    "                summary.append(f\"{tag} 加速度: データなし\")\n",
    "\n",
    "        axes[0].set_title(f'1ステップ速度誤差の低/高周波分離 (step={TARGET_STEP}, 窓={MA_WINDOW})')\n",
    "        axes[0].set_xlabel('タイムステップ')\n",
    "        axes[0].set_ylabel('速度誤差')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend(fontsize=12)\n",
    "\n",
    "        axes[1].set_title(f'1ステップ加速度誤差の低/高周波分離 (step={TARGET_STEP}, 窓={MA_WINDOW})')\n",
    "        axes[1].set_xlabel('タイムステップ')\n",
    "        axes[1].set_ylabel('加速度誤差')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend(fontsize=12)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        out_path = output_root / f'one_step_low_high_step{TARGET_STEP}.png'\n",
    "        fig.savefig(out_path, dpi=150)\n",
    "        plt.show()\n",
    "        print('分離プロットを保存:', out_path)\n",
    "        if summary:\n",
    "            print('RMS サマリ:')\n",
    "            for line in summary:\n",
    "                print(' -', line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
