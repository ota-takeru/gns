{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e989b71f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/kaggle/input/gns-codes に code も code.zip もありません。Add Data で gns-codes を追加してください。",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m     src = repo_dir\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/kaggle/input/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_SLUG\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m に code も code.zip もありません。Add Data で \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_SLUG\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m を追加してください。\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# フラット化（zip解凍で1階層挟まった場合）\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m src != repo_dir:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: /kaggle/input/gns-codes に code も code.zip もありません。Add Data で gns-codes を追加してください。"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Kaggle の入力データセット slug（username/slug の slug 部分）\n",
    "DATASET_SLUG = 'gns-codes'\n",
    "DATASET_ROOT = Path(f'/kaggle/input/{DATASET_SLUG}')\n",
    "WORK_ROOT = Path('/kaggle/working')\n",
    "repo_dir = WORK_ROOT / 'code'\n",
    "\n",
    "code_dir = DATASET_ROOT / 'code'\n",
    "code_zip = DATASET_ROOT / 'code.zip'\n",
    "\n",
    "# 展開済み code/ を優先し、無ければ code.zip を展開\n",
    "if code_dir.exists():\n",
    "    src = code_dir\n",
    "elif code_zip.exists():\n",
    "    if repo_dir.exists():\n",
    "        shutil.rmtree(repo_dir)\n",
    "    with zipfile.ZipFile(code_zip) as zf:\n",
    "        zf.extractall(repo_dir)\n",
    "    src = repo_dir\n",
    "else:\n",
    "    raise FileNotFoundError(f\"/kaggle/input/{DATASET_SLUG} に code も code.zip もありません。Add Data で {DATASET_SLUG} を追加してください。\")\n",
    "\n",
    "# フラット化（zip解凍で1階層挟まった場合）\n",
    "if src != repo_dir:\n",
    "    if repo_dir.exists():\n",
    "        shutil.rmtree(repo_dir)\n",
    "    shutil.copytree(src, repo_dir)\n",
    "if repo_dir.exists():\n",
    "    children = list(repo_dir.iterdir())\n",
    "    if len(children) == 1 and children[0].is_dir():\n",
    "        inner = children[0]\n",
    "        for p in inner.iterdir():\n",
    "            p.rename(repo_dir / p.name)\n",
    "        inner.rmdir()\n",
    "\n",
    "%cd $repo_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf889a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PyG の radius_graph で必要になる torch-cluster だけインストール\n",
    "import torch\n",
    "\n",
    "torch_ver = torch.__version__.split('+')[0]\n",
    "cuda_ver = torch.version.cuda\n",
    "cuda_tag = 'cpu' if cuda_ver is None else 'cu' + cuda_ver.replace('.', '')\n",
    "url = f\"https://data.pyg.org/whl/torch-{torch_ver}+{cuda_tag}.html\"\n",
    "print('PyG wheels:', url)\n",
    "\n",
    "!pip -q install torch-cluster -f {url}\n",
    "!pip -q install torch_geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81493ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import copy\n",
    "import yaml\n",
    "import re\n",
    "import json\n",
    "\n",
    "REPO = Path('/kaggle/working/code')\n",
    "cfg_path = REPO / 'config_rollout.yaml'\n",
    "\n",
    "# 全モデルを同じデータセットで検証したい場合のスイッチ\n",
    "USE_FORCE_DATASET = False  # True にすると下記 FORCE_DATASET_ROOT を強制使用\n",
    "FORCE_DATASET_ROOT = Path('/kaggle/input/dam-break-left-800')\n",
    "\n",
    "# モデルごとに対応するデータセットを自動解決する\n",
    "\n",
    "def _load_metadata(path: Path):\n",
    "    try:\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _find_metadata(root: Path):\n",
    "    candidates = [\n",
    "        root / 'metadata.json',\n",
    "        root / 'metadata' / 'metadata.json',\n",
    "    ]\n",
    "    candidates += list(root.glob('**/metadata*.json'))\n",
    "    for c in candidates:\n",
    "        if c.is_file():\n",
    "            meta = _load_metadata(c)\n",
    "            if isinstance(meta, dict):\n",
    "                return meta, c\n",
    "    return None, None\n",
    "\n",
    "def _extract_sequence_length(meta: dict):\n",
    "    if not isinstance(meta, dict):\n",
    "        return None\n",
    "    for key in ('train', 'valid', 'test', 'rollout'):\n",
    "        part = meta.get(key)\n",
    "        if isinstance(part, dict) and 'sequence_length' in part:\n",
    "            try:\n",
    "                return int(part['sequence_length'])\n",
    "            except Exception:\n",
    "                pass\n",
    "    if 'sequence_length' in meta:\n",
    "        try:\n",
    "            return int(meta['sequence_length'])\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def resolve_dataset_root(step: int | None):\n",
    "    base = Path('/kaggle/input')\n",
    "    if not base.exists():\n",
    "        return None, False\n",
    "\n",
    "    if USE_FORCE_DATASET:\n",
    "        if FORCE_DATASET_ROOT.exists():\n",
    "            print(f\"[force] step={step} を無視して {FORCE_DATASET_ROOT} を使用します。\")\n",
    "            return FORCE_DATASET_ROOT, True\n",
    "        else:\n",
    "            print(f\"[force] {FORCE_DATASET_ROOT} が見つかりません。通常の解決ロジックに戻ります。\")\n",
    "\n",
    "    # 1) ディレクトリ名にステップ数が含まれるものを優先\n",
    "    if step is not None:\n",
    "        for dataset_root in sorted(base.iterdir()):\n",
    "            name = dataset_root.name\n",
    "            if (name.endswith(f'-{step}') or name.endswith(f'_{step}')\n",
    "                    or f'-{step}-' in name or f'_{step}_' in name):\n",
    "                return dataset_root, False\n",
    "\n",
    "    # 2) metadata の sequence_length で一致を探す\n",
    "    for dataset_root in sorted(base.iterdir()):\n",
    "        meta, meta_path = _find_metadata(dataset_root)\n",
    "        if not meta:\n",
    "            continue\n",
    "        seq = _extract_sequence_length(meta)\n",
    "        if step is not None and seq == step:\n",
    "            return dataset_root, False\n",
    "\n",
    "    # 3) フォールバック（もっとも汎用な800データセット）\n",
    "    fallback = base / 'dam-break-left-800'\n",
    "    if fallback.exists():\n",
    "        print(f\"[fallback] step={step} に一致するデータセットが見つからず、{fallback} を使用します。\")\n",
    "        return fallback, True\n",
    "    print(f\"[fallback] step={step} に一致するデータセットも既定の dam-break-left-800 も見つかりません。\")\n",
    "    return None, True\n",
    "\n",
    "# 出力設定（共通）\n",
    "output_root = REPO / 'rollouts'\n",
    "viz_format = 'html'  # html|mp4|gif\n",
    "\n",
    "with cfg_path.open('r', encoding='utf-8') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# データセットはモデルごとに後続セルで差し替える\n",
    "cfg['method'] = 'gns'\n",
    "cfg['rollout_inference_max_examples'] = 1    #推論する数\n",
    "cfg['output_path'] = str(output_root)\n",
    "cfg.setdefault('scenario_options', {}).setdefault('fluid', {})['dataset'] = '/kaggle/input'  # placeholder\n",
    "\n",
    "# モデルは後続セルで差し替える（ベースを保存しておく）\n",
    "cfg['model_path'] = str(REPO / 'models')\n",
    "cfg['model_file'] = None\n",
    "cfg['output_filename'] = 'rollout'\n",
    "\n",
    "BASE_CFG = copy.deepcopy(cfg)\n",
    "\n",
    "with cfg_path.open('w', encoding='utf-8') as f:\n",
    "    yaml.safe_dump(cfg, f, allow_unicode=True)\n",
    "\n",
    "print('ベース設定を書き出しました:', cfg_path)\n",
    "print('output_path:', cfg['output_path'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/code\n",
    "\n",
    "import copy\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(str(Path('/kaggle/working/code/src')))\n",
    "\n",
    "from analyze_rollouts import analyze_rollout\n",
    "from simulator_factory import _get_simulator\n",
    "from train_config import load_config, INPUT_SEQUENCE_LENGTH, KINEMATIC_PARTICLE_ID\n",
    "from train_paths import _resolve_model_path\n",
    "from train_utils import _resolve_rollout_dataset_path\n",
    "import data_loader\n",
    "import reading_utils\n",
    "\n",
    "# 比較対象モデルを集約（rollout_diff 優先）\n",
    "model_roots = [\n",
    "    REPO / 'models' / 'dataset_diff',\n",
    "    REPO / 'models',\n",
    "]\n",
    "model_files: list[Path] = []\n",
    "for root in model_roots:\n",
    "    if not root.exists():\n",
    "        continue\n",
    "    for path in root.glob('*.pt'):\n",
    "        model_files.append(path.resolve())\n",
    "\n",
    "# 重複除去 & ソート（親ディレクトリ優先→ステップ番号）\n",
    "seen = set()\n",
    "unique_models: list[Path] = []\n",
    "for p in model_files:\n",
    "    if p in seen:\n",
    "        continue\n",
    "    seen.add(p)\n",
    "    unique_models.append(p)\n",
    "\n",
    "def _step_key(p: Path):\n",
    "    m = re.search(r'(\\d+)', p.stem)\n",
    "    step = int(m.group(1)) if m else -1\n",
    "    priority = 0 if p.parent.name == \"dataset_diff\" else 1\n",
    "    return (priority, p.parent.name, step)\n",
    "\n",
    "def _step_from_path(p: Path):\n",
    "    m = re.search(r'(\\d+)(?!.*\\d)', p.stem)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "model_files = sorted(unique_models, key=_step_key)\n",
    "\n",
    "if not model_files:\n",
    "    raise FileNotFoundError('*.pt が見つかりません。models/ 下にモデルを配置してください。')\n",
    "\n",
    "print('評価対象モデル:')\n",
    "for p in model_files:\n",
    "    print(' -', p)\n",
    "\n",
    "\n",
    "def compute_one_step_error(cfg_path: Path, device: torch.device):\n",
    "    # ローダの最初の軌跡のみで1-step誤差を計算\n",
    "    cfg_obj = load_config(str(cfg_path))\n",
    "    dataset_path = _resolve_rollout_dataset_path(cfg_obj)\n",
    "    if dataset_path is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"rollout/valid/test/train の npz が見つかりません: {cfg_obj.data_path}\"\n",
    "        )\n",
    "\n",
    "    metadata_key = (\n",
    "        cfg_obj.active_scenario.rollout_metadata_split\n",
    "        if getattr(cfg_obj, 'active_scenario', None)\n",
    "        and cfg_obj.active_scenario.rollout_metadata_split\n",
    "        else 'rollout'\n",
    "    )\n",
    "    metadata = reading_utils.read_metadata(cfg_obj.data_path, metadata_key)\n",
    "    if isinstance(metadata, dict) and 'acc_mean' not in metadata:\n",
    "        metadata = metadata.get('rollout') or metadata.get('train') or metadata\n",
    "    simulator = _get_simulator(\n",
    "        metadata,\n",
    "        cfg_obj.noise_std,\n",
    "        cfg_obj.noise_std,\n",
    "        device,\n",
    "        cfg_obj,\n",
    "    )\n",
    "    model_file = _resolve_model_path(cfg_obj)\n",
    "    simulator.load(model_file)\n",
    "    simulator.to(device)\n",
    "    simulator.eval()\n",
    "\n",
    "    loader = data_loader.get_data_loader_by_trajectories(dataset_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for traj in loader:\n",
    "            if len(traj) == 4:\n",
    "                positions, particle_type, material_property, n_particles = traj\n",
    "                material_property = material_property.to(device)\n",
    "            else:\n",
    "                positions, particle_type, n_particles = traj\n",
    "                material_property = None\n",
    "\n",
    "            positions = positions.to(device)\n",
    "            particle_type = particle_type.to(device)\n",
    "            n_particles_tensor = torch.tensor(\n",
    "                [int(n_particles)], device=device, dtype=torch.int32\n",
    "            )\n",
    "\n",
    "            total_steps = positions.shape[1] - INPUT_SEQUENCE_LENGTH\n",
    "            if total_steps <= 0:\n",
    "                continue\n",
    "\n",
    "            step_errors: list[float] = []\n",
    "            for t in range(total_steps):\n",
    "                window = positions[:, t : t + INPUT_SEQUENCE_LENGTH]\n",
    "                target = positions[:, t + INPUT_SEQUENCE_LENGTH]\n",
    "\n",
    "                pred = simulator.predict_positions(\n",
    "                    window,\n",
    "                    nparticles_per_example=n_particles_tensor,\n",
    "                    particle_types=particle_type,\n",
    "                    material_property=material_property,\n",
    "                )\n",
    "\n",
    "                # 運動制約粒子は GT を強制\n",
    "                kinematic_mask = (particle_type == KINEMATIC_PARTICLE_ID).to(device)\n",
    "                pred = torch.where(kinematic_mask[:, None], target, pred)\n",
    "\n",
    "                dist = torch.sqrt(((pred - target) ** 2).sum(dim=-1))\n",
    "                step_errors.append(float(dist.mean().item()))\n",
    "\n",
    "            if step_errors:\n",
    "                return {\n",
    "                    'per_timestep': step_errors,\n",
    "                    'mean_distance_error': float(np.mean(step_errors)),\n",
    "                    'n_rollouts': 1,\n",
    "                    'used_length': len(step_errors),\n",
    "                }\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "cmd = ['python', 'src/train.py', '--config', str(cfg_path)]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for model_path in model_files:\n",
    "    tag = model_path.stem  # ディレクトリ名を含めずモデル名のみ\n",
    "    step_num = _step_from_path(model_path)\n",
    "    dataset_root, used_fallback = resolve_dataset_root(step_num)\n",
    "    if dataset_root is None:\n",
    "        raise FileNotFoundError(f\"対応するデータセットが見つかりません（step={step_num}）\")\n",
    "\n",
    "    cfg = copy.deepcopy(BASE_CFG)\n",
    "    cfg['model_path'] = str(model_path.parent)\n",
    "    cfg['model_file'] = str(model_path)\n",
    "    cfg['output_filename'] = f\"rollout_{tag}\"\n",
    "    cfg.setdefault('scenario_options', {}).setdefault('fluid', {})['dataset'] = str(dataset_root)\n",
    "    cfg['data_path'] = str(dataset_root)\n",
    "    cfg['rollout_dataset'] = 'valid'\n",
    "\n",
    "    with cfg_path.open('w', encoding='utf-8') as f:\n",
    "        yaml.safe_dump(cfg, f, allow_unicode=True)\n",
    "\n",
    "    print(f\"=== {tag} ===\")\n",
    "    print('dataset:', dataset_root)\n",
    "    if used_fallback:\n",
    "        print('[warn] 指定ステップに一致するデータセットが見つからなかったためフォールバックを使用しました。')\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    output_dir = Path(cfg['output_path']) / cfg['method'] / cfg['output_filename']\n",
    "    pkl_files = sorted(output_dir.glob(f\"{cfg['output_filename']}_ex*.pkl\"))\n",
    "    if not pkl_files:\n",
    "        raise FileNotFoundError(f\"{output_dir} に *_ex*.pkl がありません。推論が成功したか確認してください。\")\n",
    "\n",
    "    # 最初の軌跡のみ使用\n",
    "    res_one = analyze_rollout(pkl_files[0])\n",
    "    dist = np.array(res_one['distance_error_per_timestep'])\n",
    "    mean_full = float(dist.mean())\n",
    "\n",
    "    one_step_res = compute_one_step_error(cfg_path, device)\n",
    "    if one_step_res:\n",
    "        print(\n",
    "            f\"  one-step (1軌跡): steps={one_step_res['used_length']} / mean distance error: {one_step_res['mean_distance_error']:.6f}\"\n",
    "        )\n",
    "    else:\n",
    "        print('  one-step: 計算できませんでした。')\n",
    "\n",
    "    results.append({\n",
    "        'tag': tag,\n",
    "        'distance_error_per_timestep': dist.tolist(),\n",
    "        'mean_distance_error': mean_full,\n",
    "        'n_rollouts': 1,\n",
    "        'used_length': len(dist),\n",
    "        'pkl_dir': str(output_dir),\n",
    "        'dataset_root': str(dataset_root),\n",
    "        'step_num': step_num,\n",
    "        'used_fallback': used_fallback,\n",
    "        'one_step_error_per_timestep': one_step_res['per_timestep'] if one_step_res else [],\n",
    "        'one_step_mean_distance_error': one_step_res['mean_distance_error'] if one_step_res else None,\n",
    "        'one_step_used_length': one_step_res['used_length'] if one_step_res else 0,\n",
    "        'one_step_n_rollouts': one_step_res['n_rollouts'] if one_step_res else 0,\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"  rollout: steps={len(dist)} / mean distance error: {mean_full:.6f} (1軌跡のみ)\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import cycle\n",
    "\n",
    "MAX_SUMMARY_STEPS = 100\n",
    "# None なら上限なし\n",
    "MAX_PLOT_STEPS = None  # 1ステップ誤差の表示上限\n",
    "\n",
    "# ラベル・凡例のサイズ設定（大きめ）\n",
    "LABEL_FONTSIZE = 16\n",
    "TICK_FONTSIZE = 14\n",
    "LEGEND_FONTSIZE = 16\n",
    "TITLE_FONTSIZE = 18\n",
    "\n",
    "if not results:\n",
    "    print('results が空です。前のセルを実行してください。')\n",
    "else:\n",
    "    # モデルごとに固定の色を割り当て（rollout と 1step で揃える）\n",
    "    color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get('color', [])\n",
    "    if not color_cycle:\n",
    "        color_cycle = list(plt.cm.tab10.colors)\n",
    "    color_iter = cycle(color_cycle)\n",
    "    color_map = {r['tag']: next(color_iter) for r in results}\n",
    "\n",
    "    # -------- 全タイムステップの距離誤差（rollout） --------\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    max_len_rollout = max(len(np.array(r['distance_error_per_timestep'])) for r in results)\n",
    "    for r in results:\n",
    "        err = np.array(r['distance_error_per_timestep'])\n",
    "        timesteps = np.arange(len(err))\n",
    "        # 短い系列ほど zorder を高くして上に重ねる\n",
    "        z = max_len_rollout - len(err)\n",
    "        plt.plot(\n",
    "            timesteps,\n",
    "            err,\n",
    "            label=f\"{r['tag']} (T={len(err)})\",\n",
    "            color=color_map[r['tag']],\n",
    "            zorder=z,\n",
    "        )\n",
    "    plt.xlabel('timestep', fontsize=LABEL_FONTSIZE)\n",
    "    plt.ylabel('mean distance error', fontsize=LABEL_FONTSIZE)\n",
    "    plt.title('Rollout distance error per timestep (single trajectory)', fontsize=TITLE_FONTSIZE)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tick_params(labelsize=TICK_FONTSIZE)\n",
    "    plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = output_root / 'distance_error_comparison.png'\n",
    "    plt.savefig(plot_path, dpi=150)\n",
    "    plt.show()\n",
    "    print('プロットを保存:', plot_path)\n",
    "\n",
    "    # -------- 1ステップ誤差（教師あり1-step, single trajectory） --------\n",
    "    plot_items = []\n",
    "    for r in results:\n",
    "        err = np.array(r.get('one_step_error_per_timestep') or [])\n",
    "        if err.size == 0:\n",
    "            continue\n",
    "        if MAX_PLOT_STEPS is not None:\n",
    "            err = err[:MAX_PLOT_STEPS]\n",
    "        # results の順序を維持して rollouts と同じ並びにする\n",
    "        plot_items.append((len(err), err, r))\n",
    "\n",
    "    if plot_items:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        max_len_onestep = max(len(item[1]) for item in plot_items)\n",
    "        for _, err, r in plot_items:\n",
    "            timesteps = np.arange(len(err))\n",
    "            z = max_len_onestep - len(err)\n",
    "            plt.plot(\n",
    "                timesteps,\n",
    "                err,\n",
    "                marker='o',\n",
    "                markersize=3,\n",
    "                linewidth=1.2,\n",
    "                label=f\"{r['tag']} (T={len(err)})\",\n",
    "                color=color_map[r['tag']],\n",
    "                zorder=z,\n",
    "            )\n",
    "        plt.xlabel('timestep', fontsize=LABEL_FONTSIZE)\n",
    "        plt.ylabel('mean distance error (1-step, teacher forcing)', fontsize=LABEL_FONTSIZE)\n",
    "        max_label = 'all' if MAX_PLOT_STEPS is None else MAX_PLOT_STEPS\n",
    "        plt.title(f'1-step error per timestep (up to {max_label})', fontsize=TITLE_FONTSIZE)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tick_params(labelsize=TICK_FONTSIZE)\n",
    "        plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_path_step = output_root / 'distance_error_per_timestep.png'\n",
    "        plt.savefig(plot_path_step, dpi=150)\n",
    "        plt.show()\n",
    "        print('1ステップ誤差プロットを保存:', plot_path_step)\n",
    "    else:\n",
    "        print('one-step 誤差を描画できるデータがありません。')\n",
    "\n",
    "    # -------- サマリ --------\n",
    "    print('サマリ（平均距離誤差と長さ: いずれも1軌跡のみ）')\n",
    "    for r in results:\n",
    "        err = np.array(r['distance_error_per_timestep'])\n",
    "        mean_full = float(err.mean())\n",
    "        mean_100 = float(err[: min(MAX_SUMMARY_STEPS, len(err))].mean())\n",
    "        print(f\"- {r['tag']}: rollout steps={len(err)}, mean_full={mean_full:.6f}, mean@<=100={mean_100:.6f}\")\n",
    "\n",
    "        err1 = np.array(r.get('one_step_error_per_timestep') or [])\n",
    "        if err1.size:\n",
    "            mean_full1 = float(err1.mean())\n",
    "            mean_1001 = float(err1[: min(MAX_SUMMARY_STEPS, len(err1))].mean())\n",
    "            print(\n",
    "                f\"    1-step: steps={len(err1)}, mean_full={mean_full1:.6f}, mean@<=100={mean_1001:.6f}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "DT = 0.006  # dam-break-left 系列のシミュレーション刻み\n",
    "TARGET_NAME = 'dam-break-left-800'\n",
    "\n",
    "if not results:\n",
    "    print('results が空です。前のセルを実行してください。')\n",
    "else:\n",
    "    target = [r for r in results if TARGET_NAME in str(r.get('dataset_root', ''))]\n",
    "    if not target:\n",
    "        print(f\"{TARGET_NAME} を含むデータセットの結果が見つかりません。先に FORCE を有効化するか、対象モデルだけ実行してください。\")\n",
    "    else:\n",
    "        color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get('color', []) or list(plt.cm.tab10.colors)\n",
    "        color_map = {r['tag']: color_cycle[i % len(color_cycle)] for i, r in enumerate(target)}\n",
    "\n",
    "        def _load_rms(pkl_path: Path):\n",
    "            with pkl_path.open('rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            pred = data['predicted_rollout']\n",
    "            gt = data['ground_truth_rollout']\n",
    "            if pred.ndim == 4:\n",
    "                pred = pred[:, 0]\n",
    "            if gt.ndim == 4:\n",
    "                gt = gt[:, 0]\n",
    "            v_pred = np.diff(pred, axis=0) / DT\n",
    "            v_gt = np.diff(gt, axis=0) / DT\n",
    "            a_pred = np.diff(v_pred, axis=0) / DT\n",
    "            a_gt = np.diff(v_gt, axis=0) / DT\n",
    "            v_rms_pred = np.sqrt((v_pred ** 2).mean(axis=(1, 2)))\n",
    "            v_rms_gt = np.sqrt((v_gt ** 2).mean(axis=(1, 2)))\n",
    "            a_rms_pred = np.sqrt((a_pred ** 2).mean(axis=(1, 2)))\n",
    "            a_rms_gt = np.sqrt((a_gt ** 2).mean(axis=(1, 2)))\n",
    "            return v_rms_pred, v_rms_gt, a_rms_pred, a_rms_gt\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "        for r in target:\n",
    "            pdir = Path(r['pkl_dir'])\n",
    "            pkls = sorted(pdir.glob('*_ex*.pkl'))\n",
    "            if not pkls:\n",
    "                print('pkl が見つかりません:', pdir)\n",
    "                continue\n",
    "            v_pred, v_gt, a_pred, a_gt = _load_rms(pkls[0])\n",
    "            t_v = np.arange(len(v_pred))\n",
    "            t_a = np.arange(len(a_pred))\n",
    "            color = color_map[r['tag']]\n",
    "            axes[0].plot(t_v, v_gt, linestyle='--', color=color, alpha=0.8, label=f\"{r['tag']} GT\")\n",
    "            axes[0].plot(t_v, v_pred, linestyle='-', color=color, label=f\"{r['tag']} Pred\")\n",
    "            axes[1].plot(t_a, a_gt, linestyle='--', color=color, alpha=0.8, label=f\"{r['tag']} GT\")\n",
    "            axes[1].plot(t_a, a_pred, linestyle='-', color=color, label=f\"{r['tag']} Pred\")\n",
    "\n",
    "        axes[0].set_ylabel('velocity RMS')\n",
    "        axes[0].set_title(f'Velocity RMS over time ({TARGET_NAME})')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[1].set_xlabel('timestep')\n",
    "        axes[1].set_ylabel('acceleration RMS')\n",
    "        axes[1].set_title(f'Acceleration RMS over time ({TARGET_NAME})')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        handles, labels = axes[0].get_legend_handles_labels()\n",
    "        axes[0].legend(handles, labels, loc='upper right')\n",
    "        handles, labels = axes[1].get_legend_handles_labels()\n",
    "        axes[1].legend(handles, labels, loc='upper right')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        out_path = output_root / f'rms_{TARGET_NAME}.png'\n",
    "        fig.savefig(out_path, dpi=150)\n",
    "        plt.show()\n",
    "        print('RMS プロットを保存:', out_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}