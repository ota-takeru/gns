{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNS Kaggle Pipeline Notebook\n",
    "\n",
    "Edit this notebook locally, keep it under version control, and push it to Kaggle when you want to execute on their remote hardware. Attach this repository (zipped or as a Kaggle Dataset) to the Kaggle Notebook so all relative paths stay valid.\n",
    "\n",
    "**Local -> Kaggle workflow**\n",
    "1. Update the repo locally, including this notebook.\n",
    "2. Publish the updated sources (e.g. `kaggle notebooks push -p gns_kaggle` from the project root, or ship a new Dataset version).\n",
    "3. Open the Notebook on Kaggle and run the cells from top to bottom.\n",
    "\n",
    "Outputs are written to `kaggle_models/` (checkpoints) and `kaggle_rollouts/` (inference results) so they can be collected as Notebook outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the pipeline\n",
    "Toggle the flags below to control which stages run during the Kaggle job. The default configuration executes the full flow (dependency install -> dataset generation -> training -> rollout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inside Kaggle: True\n",
      "Working directory: /kaggle/working\n",
      "Dataset config: datasets/config/fluid_kaggle.yaml [MISSING]\n",
      "Train config: config_kaggle.yaml [MISSING]\n",
      "Rollout config: config_kaggle_rollout.yaml [MISSING]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IN_KAGGLE = Path(\"/kaggle\").exists()\n",
    "print(f\"Running inside Kaggle: {IN_KAGGLE}\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Pipeline toggles ---------------------------------------------------------\n",
    "SKIP_INSTALL = True\n",
    "FORCE_REINSTALL = False\n",
    "SKIP_GENERATE = False\n",
    "SKIP_TRAIN = False\n",
    "SKIP_ROLLOUT = False\n",
    "RUN_ANALYSIS = False\n",
    "VISUALIZE_HTML = False\n",
    "\n",
    "# Configuration files ------------------------------------------------------\n",
    "DATASET_CONFIG = Path(\"datasets/config/fluid_kaggle.yaml\")\n",
    "TRAIN_CONFIG = Path(\"config_kaggle.yaml\")\n",
    "ROLLOUT_CONFIG = Path(\"config_kaggle_rollout.yaml\")\n",
    "\n",
    "for label, cfg in (\n",
    "    (\"Dataset\", DATASET_CONFIG),\n",
    "    (\"Train\", TRAIN_CONFIG),\n",
    "    (\"Rollout\", ROLLOUT_CONFIG),\n",
    "):\n",
    "    status = \"OK\" if cfg.exists() else \"MISSING\"\n",
    "    print(f\"{label} config: {cfg} [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the orchestration script\n",
    "This cell wraps `gns_kaggle/pipeline.py`, which already knows how to install dependencies and invoke the training scripts with the lightweight Kaggle-friendly settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gns_kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_406/1206633919.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgns_kaggle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgns_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gns_kaggle'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from gns_kaggle import pipeline as gns_pipeline\n",
    "\n",
    "\n",
    "def _ensure(path: Path, label: str) -> Path:\n",
    "    resolved = path.expanduser().resolve()\n",
    "    if not resolved.exists():\n",
    "        raise FileNotFoundError(f\"{label} not found: {resolved}\")\n",
    "    return resolved\n",
    "\n",
    "\n",
    "if not SKIP_INSTALL:\n",
    "    gns_pipeline.install_dependencies(force=FORCE_REINSTALL)\n",
    "else:\n",
    "    print(\"[notebook] Skipping dependency installation.\")\n",
    "\n",
    "if not SKIP_GENERATE:\n",
    "    gns_pipeline.generate_dataset(_ensure(DATASET_CONFIG, \"Dataset config\"))\n",
    "else:\n",
    "    print(\"[notebook] Skipping dataset generation.\")\n",
    "\n",
    "if not SKIP_TRAIN:\n",
    "    gns_pipeline.train_model(_ensure(TRAIN_CONFIG, \"Training config\"))\n",
    "else:\n",
    "    print(\"[notebook] Skipping training.\")\n",
    "\n",
    "if not SKIP_ROLLOUT:\n",
    "    gns_pipeline.run_rollout(_ensure(ROLLOUT_CONFIG, \"Rollout config\"))\n",
    "    if RUN_ANALYSIS:\n",
    "        gns_pipeline.analyze_rollouts()\n",
    "    if VISUALIZE_HTML:\n",
    "        gns_pipeline.visualize_rollouts(html=True)\n",
    "else:\n",
    "    print(\"[notebook] Skipping rollout inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect generated artifacts\n",
    "Quickly list the checkpoint and rollout directories so you can decide what to keep as Kaggle Notebook outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def list_directory(path: Path) -> None:\n",
    "    if not path.exists():\n",
    "        print(f\"{path}: (missing)\")\n",
    "        return\n",
    "    print(f\"{path}:\")\n",
    "    for child in sorted(path.iterdir()):\n",
    "        if child.is_dir():\n",
    "            marker = \"<DIR>\"\n",
    "        else:\n",
    "            marker = f\"{child.stat().st_size / 1024:.1f} KiB\"\n",
    "        print(f\"  {child.name:30s} {marker}\")\n",
    "\n",
    "\n",
    "list_directory(Path(\"kaggle_models\"))\n",
    "list_directory(Path(\"kaggle_rollouts\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package outputs (optional)\n",
    "Copy the important directories into a single export folder and create a zip archive that can be downloaded from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "ARTIFACT_ROOT = Path(\"kaggle_export\")\n",
    "ARTIFACT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "sources = (Path(\"kaggle_models\"), Path(\"kaggle_rollouts\"))\n",
    "copied = []\n",
    "for source in sources:\n",
    "    target = ARTIFACT_ROOT / source.name\n",
    "    if target.exists():\n",
    "        if target.is_dir():\n",
    "            shutil.rmtree(target)\n",
    "        else:\n",
    "            target.unlink()\n",
    "    if source.exists():\n",
    "        if source.is_dir():\n",
    "            shutil.copytree(source, target)\n",
    "        else:\n",
    "            shutil.copy2(source, target)\n",
    "        copied.append(target)\n",
    "        print(f\"Copied {source} -> {target}\")\n",
    "    else:\n",
    "        print(f\"Skipped missing source: {source}\")\n",
    "\n",
    "zip_path = Path(\"gns_artifacts.zip\")\n",
    "if zip_path.exists():\n",
    "    zip_path.unlink()\n",
    "\n",
    "archive = shutil.make_archive(\"gns_artifacts\", \"zip\", root_dir=ARTIFACT_ROOT)\n",
    "archive_path = Path(archive)\n",
    "size_kib = archive_path.stat().st_size / 1024\n",
    "print(f\"Created archive: {archive_path} ({size_kib:.1f} KiB)\")\n",
    "if IN_KAGGLE:\n",
    "    print(\"Add gns_artifacts.zip to the Notebook output files before finishing the run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
